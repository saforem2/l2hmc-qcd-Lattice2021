%!TEX program = xelatex
% Please make sure you insert your
% data according to the instructions in PoSauthmanual.pdf
\documentclass[a4paper,11pt]{article}
\usepackage{pos}
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{silence}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
% \reversemarginpar

\definecolor{blue}{HTML}{007fff}
\definecolor{red}{HTML}{FF2052}

\newcommand{\mask}{\textcolor{blue}{{m}}}
\newcommand{\maskbar}{\textcolor{red}{\bar{m}}}

\newcommand{\xmask}{\textcolor{blue}{{x}_{m}}}
\newcommand{\xmaskbar}{\textcolor{red}{x_{\bar{m}}}}

\newcommand{\xpmask}{\textcolor{blue}{{x'}_{m}}}
\newcommand{\xpmaskbar}{\textcolor{red}{x'_{\bar{m}}}}

\newcommand{\mbart}{\textcolor{red}{\bar{m}^{k}}}
\newcommand{\mt}{\textcolor{blue}{m^{k}}}
\newcommand{\gradS}{\partial_{x} S(x)}
\newcommand{\acceptProb}{%
            A(\xi^{\ast}|\xi) \equiv \min\left\{1,
            \frac{p(\xi^{\ast})}{p(\xi)}%
        \left|\frac{\partial \xi^{\ast}}{\partial \xi^{T}}\right|\right\}
    }

\newcommand{\mpar}[1]{\marginpar{\color{red}{#1}}}

\title{LeapfrogLayers: A Trainable Framework for Effective Topological Sampling}
% \ShortTitle{A Trainable Framework for effective Topological Sampling}

\author*[a]{Sam Foreman}
\author[a]{Xiao-Yong Jin}
\author[a]{James C. Osborn}

\affiliation[a]{Argonne National Laboratory,\\
  Lemont, IL}

% \affiliation[b]{Department, University,\\
% Street number, City, Country}

\emailAdd{foremans@anl.gov}
\emailAdd{xjin@anl.gov}
\emailAdd{osborn@alcf.anl.gov}

\abstract{
    We introduce LeapfrogLayers, an invertible neural network architecture that
    can be trained to efficiently sample the topology of a 2D $U(1)$ lattice
    gauge theory. 
    %
    We show an improvement in the integrated autocorrelation time of the
    topological charge when compared with traditional HMC, and propose methods
    for scaling our model to larger lattice volumes.
}

\FullConference{
  The 38th International Symposium on Lattice Field Theory\\
  26-30 July 2021\\
  Zoom / Gather @ MIT, Cambridge MA, USA\\
}

%% \tableofcontents
\begin{document}
\maketitle


\section{\label{sec:intro}Introduction}
%
\marginpar{\textcolor{red}{Background}}
%
One of the major goals of lattice field theory calculations is to evaluate
integrals of the form
%
\begin{equation}
    \langle \mathcal{O} \rangle \propto \int \left[ \mathcal{D} x \right] \mathcal{O}(x) p(x),
    \label{eq:density_integral}
\end{equation}
%
for some target distribution \(p(x)\propto e^{-S(x)}\).
%
We can approximate the integral using Markov Chain Monte Carlo (MCMC) sampling
techniques.
%
This is done by sequentially generating a chain of configurations \(\{x_{1},
x_{2}, \ldots x_{N}\}\), with \(x_{i} \sim p(x)\) and averaging the value of
the function \(\mathcal{O}(x)\) over the chain.
%
Accounting for correlations between states in our chain, the sampling variance
of this esimator is given by
%
\begin{equation}
    \sigmaup^{2} = \frac{\tau_{\mathcal{O}}^{\mathrm{int}}}{N}\sum_{n=1}^{N} \mathrm{Var}\left[\mathcal{O}(x)\right]
\end{equation}
%
% TODO: Elaborate on this
where \(\tau_{\mathrm{int}}^{\mathcal{O}}\) is the integrated autocorrelation
time.
%
This quantity can be interpreted as the additional time required for
these induced correlations to become negligible.
%
\subsection{\label{subsec:qfreezing}Charge Freezing}
%
\begin{wrapfigure}[19]{r}[36pt]{0.53\textwidth}
    \vspace{-\baselineskip}
    \centering
    \includegraphics[width=0.53\textwidth]{assets/qfreezing.pdf}
    \caption{\label{fig:qfreezing}Illustration of the topological charge \(Q\)
    freezing as \(\beta : 2 \rightarrow 7\) for traditional HMC.}
\end{wrapfigure}
%
The ability to efficiently generate independent configurations is currently
a major bottleneck for lattice simulations.
%
In this work we consider a \(U(1)\) gauge model on a 2D lattice with periodic
boundary conditions.
%
The theory is defined in terms of the link variables \(U_{\mu}(x) = e^{i
x_{\mu}(n)} \in U(1)\) with \(x_{\mu}(n) \in [-\pi, \pi]\).
%
Our target distribution is given by \(p(x)\propto e^{-S_{\beta}(x)}\), where
\(S_{\beta}(x)\) is the Wilson action
%
\begin{align}
    S_{\beta}(x) &= \beta \sum_{P} 1 - \cos{x_{P}}, \text{ and}\\
    x_{P} &\equiv \big[x_{\mu}(n) + x_{\nu}(n + \hat{\mu})\nonumber \\
          &\quad\quad\,\, - x_{\mu}(n+\hat{\nu}) - x_{\nu}(n)\big],\nonumber
\end{align}
%
is the sum of the gauge variables around the elementary plaquette.
%
For a given lattice configuration, we can calculate the topological charge \(Q
\in \mathbb{Z}\) using
%
\begin{align*}
    Q_{\mathbb{Z}} &= \frac{1}{2\pi}\sum_{P}\left\lfloor x_{P} \right\rfloor,\text{ where } \\
    \left\lfloor x_{P} \right\rfloor &\equiv x_{P} - 2\pi
    \left\lfloor\frac{x_{P}+\pi}{2\pi}\right\rfloor.
\end{align*}
%
As \(\beta \rightarrow \infty\), the \(Q_{\mathbb{Z}} = 0\) mode becomes
dominant and we see that the value of \(Q_\mathbb{Z}\) remains fixed for large
durations of the simulation.
%

This can be seen by introducing the \emph{tunneling rate}\footnote{As measured
between subsequent states in our chain \(i\), \(i+1\).}
%
\begin{equation}
    \delta Q \equiv \left|Q_{i+1} - Q_{i}\right| \in \mathbb{Z}.
\end{equation}
%
This quantity serves as a measure for how efficiently our chain is able to jump
(tunnel) between sectors of distinct topological charge.
%
From Fig~\ref{fig:qfreezing}, we can see that \(\delta Q\rightarrow 0\) as
\(\beta\rightarrow \infty\).
%
\section{\label{sec:hmc}Hamiltonian Monte Carlo (HMC)}
%
% We first briefly review the ideas of the Hamiltonian Monte Carlo (HMC)
% algorithm and highlight some of the issues faced with this approach.
%
The Hamiltonian Monte Carlo algorithm begins by introducing a fictitious
momentum \(v \sim \mathcal{N} (0, \mathbbm{1})\) distributed independently of
\(x\).
%
This allows us to write the joint target density of the \(\xi \equiv (x, v)\)
system as
%
\begin{equation}
    p(x, v) = p(x) \cdot p(v) = e^{-S_{\beta}(x)}\cdot e^{-v^{T} v / 2} = e^{-H(x, v)}
\end{equation}
%
where \(H(\xi) = H(x, v) = S_{\beta}(x) + \frac{1}{2} v^{T} v\) is the
Hamiltonian of the system.
%
We use the \emph{leapfrog integrator} to numerically integrate Hamilton's
equations
%
\begin{equation}
    \dot{x} = \frac{\partial H}{\partial v}\quad \text{,} \quad \dot{v} = - \frac{\partial H}{\partial x}
\end{equation}
%
along iso-probability contours of \(H =\text{const.}\) from \(\xi = (x,
v)\rightarrow (x^{\ast}, v^{\ast}) = \xi^{\ast}\).
%
\subsection{\label{subsec:lfint}Leapfrog Integrator}
%
\begin{enumerate}
    \item Starting from \(x_{0}\), resample the momentum \(v_{0}\sim
        \mathcal{N} (0, \mathbbm{1})\) and construct the state \(\xi_{0} =
        (x_{0}, v_{0})\).
    \item Generate a \emph{proposal configuration} \(\xi^{\ast}\) by
        integrating \(\dot\xi\) along \(H = \mathrm{const.}\)
        for \(N\) leapfrog steps.
        i.e.
        \begin{equation}
            \xi_{0}\rightarrow \xi_{1}\rightarrow\ldots\rightarrow
            \xi_{N} \equiv \xi^{\ast},
        \end{equation}
        where a single leapfrog step \(\xi_{i} \rightarrow \xi_{i+1}\) above
        consists of: 
        %
        \begin{equation}
            \textbf{ (a.) }%
              \tilde{v}\leftarrow v - \frac{\varepsilon}{2}\partial_{x} S(x),
            \quad\quad
            \textbf{ (b.) }%
              x' \leftarrow x + \varepsilon \tilde{v},
            \quad\quad
            \textbf{ (c.) }%
              v' \leftarrow \tilde{v} - \frac{\varepsilon}{2}\partial_{x} S(x).
        \end{equation}
        %
    \item At the end of the trajectory, accept or reject the proposal
        configuration \(\xi^{\ast}\) using the Metropolis-Hastings (MH) test
        \begin{equation}
            x_{i+1} \leftarrow
            \begin{cases}
                x^{\ast}\text{ with probability } \acceptProb \\
                x_{i}\text{ with probability } 1 - A(\xi^{\ast}|\xi).
            \end{cases}
        \end{equation}
\end{enumerate}
%
An illustration of this procedure can be seen in Fig~\ref{fig:hmc}.
%
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{assets/hmc.pdf}
    \caption{\label{fig:hmc}High-level overview of the HMC algorithm.}
\end{figure}
%
\subsection{\label{subsec:hmc_issues}Issues with HMC}
%
Re-sampling the momentum at the start of each trajectory causes the energy
levels we explore to be randomly selected.
%
This is reminiscient of the ``random-walk'' behavior of traditional MCMC and
leads to a slow exploration of our target distribution (i.e. long
autocorrelations).
%
Additionally, the HMC sampler is known to have difficulty traversing
low-density zones, resulting in poor performance for distributions which have
multiple isolated modes.
%
This is particularly relevant in the case of sampling topological quantities in
lattice gauge models.
%
%
% This list of criteria helps to illuminate the qualities we would like our
% (ideal) sampler to exhibit; namely, we would like for our sampler to: 
% %
% \begin{itemize}
%     \item quickly converge to the true target distribution
%     \item mix quickly once converged (short autocorrelations)
%     \item be able to mix across energy levels and isolated modes
% \end{itemize}
%
\section{\label{sec:l2hmc}Generalizing HMC: LeapfrogLayers}
%
In order to preserve the asymptotic behavior of HMC, our update must be
explicitly reversible with a tractable Jacobian determinant.
%
% \begin{enumerate}
%     \item Be invertible / preserve reversibility, i.e. \(p(a\rightarrow b) =
%         p(b\rightarrow a)\)
%     \item Have a tractable Jacobian determinant.
% \end{enumerate}
% %
To simplify notation, we introduce two functions, \(\Gamma\) \((\Lambda)\) to
denote the \(v\) \((x)\) updates.
%
As in HMC, we follow the general pattern of performing alternating updates of
\(v\) and \(x\).
%

We can ensure our update is reversible by splitting the \(x\)-update into two
parts and sequentially updating complementary subsets using a binary mask
\(\mask\) and its complement \(\maskbar\).
%
Additionally, we introduce \(d \sim \mathcal{U} (+, -)\), distributed
independently of both \(x\) and \(v\), to determine the ``direction'' of our
update\footnote{%
  As indicated by the superscript \(\pm\) on \(\Gamma^{\pm}, \Lambda^{\pm}\) in
  the update functions.
}.
%
Here, we associate \(+\) \((-)\) with the forward (backward) direction and note
that running sequential updates in opposite directions has the effect of
inverting the update.
%
We denote the complete state by \(\xi = (x, v, \pm)\), with target density
given by \(p(\xi) = p(x)\cdot p(v)\cdot p(\pm)\).
%

Explicitly, we can write this series of updates as\footnote{%
  Here we denote by \(\xmask = \mask \odot x\) and 
  \(\xmaskbar = \maskbar \odot x\) with \(\mathbbm{1} = \mask + \maskbar\).
}
%
\begin{align}
    \mathbf{(1.)}\,\,\,\, v' 
        &= \Gamma^{\pm}\left[v;\, \zeta_{v} \right] \\
    \mathbf{(2.)}\,\,\,\, x' 
        &= \mask \odot x + \maskbar \odot \Lambda^{\pm} [\xmaskbar; \zeta_{\bar{x}}] \\
    \mathbf{(3.)}\,\, x'' 
        &= \maskbar \odot x' + \mask \odot \Lambda^{\pm} [\xmask; \zeta_{x'}] \\
    \mathbf{(4.)}\,\, v'' 
        &= \Gamma^{\pm} [v', \zeta_{v'}]
\end{align}
%
where \(\zeta_{\bar{x}} = [\maskbar \odot x, v]\), \(\zeta_{x} = [\mask\odot x,
v]\) \((\zeta_{v} = [x, \partial_{x} S(x)])\) is independent of \(x\) \((v)\)
and is passed as input to the update functions \(\Lambda^{\pm}\)
\((\Gamma^{\pm})\).
%
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{assets/update_steps.pdf}
        \caption{\label{subfig:updates}Generalized leapfrog update.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.58\textwidth}
        \includegraphics[width=\textwidth]{assets/leapfrog_layer.pdf}
        \caption{\label{subfig:lfupdate}Schematic diagram showing the data flow
        through a leapfrog layer.}
    \end{subfigure}
    \vspace{1em}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{assets/network_functions.pdf}
        \caption{\label{subfig:network_fns}Detailed view of the first two steps
        in Fig~\ref{subfig:updates}.}
    \end{subfigure}
    \caption{\label{fig:networks}Various illustrations of the generalized
        leapfrog update. Note that \(k\) indexes the leapfrog step along our
    trajectory.}
\end{figure}
%
%     of the leapfrog layer used
%     to generate a proposal configuration \(\xi\rightarrow \xi''\) which is
% then either accepted or rejected.}
%     which uses invertible neural networks to augment the
% traditional update.}
%
\subsection{\label{subsec:networks}Network Details}
%
Normalizing flows~\cite{dinhDensityEstimationUsing2016b} are an obvious choice
for the structure of the update functions.
%
These architectures are easily invertible while maintaining a tractable
Jacobian determinant, and have also been shown to be effective at approximating
complicated target distributions in high-dimensional spaces ~\cite{%
    foremanDeepLearningHamiltonian2021a,
    dinhDensityEstimationUsing2016b,
    albergoFlowbasedGenerativeModels2019b,%
    boydaSamplingUsingSU2020a,%
    kanwarEquivariantFlowbasedSampling2020a,%
    wehenkelYouSayNormalizing2020a,%
    levyGeneralizingHamiltonianMonte2018b,%
    neklyudovOrbitalMCMC2020a,%
    neklyudovInvolutiveMcmcUnifying2020a%
},

Note that we maintain separate networks \(\Gamma\), \(\Lambda\) with
identical architectures for updating \(v\) and \(x\), respectively.
%
Without loss of generality, we describe below the details of the \(x\)-update
for the forward \((d = +)\) direction, \(\Lambda^{+}[\xmaskbar;
\zeta_{\bar{x}}]\)\footnote{
    To obtain the expression for the \(d=-\) direction we simply invert the
    update function and perform the updates in opposite order.
}.
%
For simplicity, we describe the data flow through a single leapfrog layer,
%
which takes as input \(\zeta_{\bar{x}} = (\xmaskbar, v)\). For the 2D \(U(1)\)
model, the gauge links are encoded as \([\cos(x), \sin(x)]\) for \(x \in [-\pi,
\pi]\).
%
Explicitly\footnote{
    Here \(\sigma(\cdot)\) is a generic nonlinear activation function.
},
%
\begin{align}
    h_{1} &= \sigma\left(w_{x}^{T} x + w_{v}^{T} v + b_{1}\right)
        \quad \in \mathbb{R}^{n_{1}} \\
    h_{2} &= \sigma\left(w_{2}^{T} h_{1} + b_{2}\right)
        \hspace{0.085\textwidth} \in \mathbb{R}^{n_{2}} \\
    \nonumber& \vdots\\
    h_{k} &= \sigma\left(w_{k}^{T} h_{k-1} + b_{k-1}\right)
        \hspace{0.04\textwidth} \in \mathbb{R}^{n_{k}} \Longrightarrow \\
    \nonumber
    \textbf{  [1.]  } s_{x} = \lambda_{s} \tanh \left( w^{T}_{s} h_{k} + b_{s} \right);
    &\quad\textbf{  [2.]  } t_{x} = w^{T}_{t} h_{k} + b_{t};
    \quad\textbf{  [3.]  } q_{x} = \lambda_{q}\tanh\left(w^{T}_{q} h_{k} + b_{q}\right);
\end{align}
%
where the outputs \(s_{x}, t_{x}, q_{x}\) are of the same dimensionality as
\(x\), and \(\lambda_{s}, \lambda_{q}\) are trainable parameters.
%
Note that the shapes of the hidden layers \(h_{k}\) are determined by the sizes
of each of the weight matrices \(w_{k}^{T}\).
%
These outputs are then used to update \(x\), as shown in
Fig~\ref{fig:networks}.

\subsection{\label{subsec:trainstep}Training Step}
%
\begin{enumerate}
    \item Resample \(v \sim \mathcal{N} (0, \mathbbm{1})\),
        \(d \sim \mathcal{U} (+, -)\), and construct initial state
        \(\xi_{0} = (x_{0}, v_{0}, \pm)\)
    \item Generate the proposal configuration \(\xi^{\ast}\) by passing the
        initial state sequentially through \(N\) \emph{leapfrog
        layers }%\footnote{
            %We defer the details to Sec~\ref{sec:l2hmc}.
        %}
        to construct a trajectory: \(\xi_{0} \rightarrow \xi_{1} \rightarrow
        \ldots \rightarrow \xi_{N} = \xi^{\ast}\)
    \item Compute the Metropolis-Hastings acceptance \(A(\xi^{\ast} | \xi) =
        \min\left\{1, \frac{p(\xi^{\ast})}{p(\xi)}\left|\frac{\partial
        \xi'}{\partial \xi^{T}} \right| \right\}\)
    \item Evaluate the loss function \(\mathcal{L} \leftarrow
        \mathcal{L}_{\theta}(\xi^{\ast}, \xi)\), and backpropagate gradients to
        update weights
    \item Evaluate Metropolis-Hastings criteria and assign the next state in
        the chain according to
        \(x_{t+1} \leftarrow \begin{cases}%
            x^{\ast} \text{ with prob. } A(\xi^{\ast}|\xi) \\
            x \text{ with prob. } 1 - A(\xi^{\ast}|\xi).
        \end{cases}\)
\end{enumerate}
%
\section{\label{sec:results}Results}
We can measure the performance of our approach by comparing the integrated
autocorrelation time against traditional HMC.
%
We see in Fig~\ref{fig:autocorr} that our estimate of the integrated
autocorrelation time is much shorter for the trained model across \(\beta \in
[2, 7]\).
%
\begin{figure}[htpb]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{assets/autocorr_vs_beta.pdf}
        \caption{\label{fig:autocorr} Plot of the integrated autocorrelation
        time \(\tau_{\mathrm{int}}^{Q}\) of the topological charge for both HMC
    (black, dashed line) and the trained model (solid, red line).}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{assets/plaqsf_beta.pdf}
        \caption{\label{fig:plaqsf_beta}Deviation in the average plaquette vs
        leapfrog step.}
    \end{subfigure}
\end{figure}
%
To better understand how these transformations effect the HMC update, we can
look at how various quantities evolve over the course of a trajectory, as shown
in Fig~\ref{fig:ridgeplots}.
%
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{assets/ridgeplots.pdf}
    \caption{\label{fig:ridgeplots}Evolution of the density of lattice
    observables over the course of a trajectory.}
\end{figure}

% Explicitly, if we denote by \(M^{\pm} = \left(\begin{smallmatrix}\Gamma^{\pm} &
% \cdot \\ \cdot & \Lambda^{\pm}\end{smallmatrix}\right)\), we have that 
% %
% \begin{equation}
%     \xi^{\pm} = M^{\pm}\xi \Longrightarrow \xi = M^{\mp}\xi^{\pm}.
% \end{equation}
% %

% \begin{thebibliography}{98}
\bibliography{main}
\bibliographystyle{plain}
% \bibitem{...}

% \end{thebibliography}

\end{document}
