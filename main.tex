%! TEX program = xelatex
% Please make sure you insert your
% data according to the instructions in PoSauthmanual.pdf
\documentclass[a4paper,11pt]{article}
\usepackage{pos}
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[ruled, vlined]{algorithm2e}
\usepackage{silence}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
% \reversemarginpar
\newcommand{\mpar}[1]{\marginpar{\color{red}{#1}}}

\definecolor{blue}{HTML}{007fff}
\definecolor{red}{HTML}{FF2052}

\newcommand{\mask}{\textcolor{blue}{m}}
\newcommand{\maskbar}{\textcolor{red}{\bar{m}}}

\newcommand{\mbart}{\textcolor{red}{\bar{m}^{k}}}
\newcommand{\mt}{\textcolor{blue}{m^{k}}}

\title{LeapfrogLayers: A Trainable Framework for Effective Topological Sampling}
% \ShortTitle{A Trainable Framework for effective Topological Sampling}

\author*[a]{Sam Foreman}
\author[a]{Xiao-Yong Jin}
\author[a]{James C. Osborn}

\affiliation[a]{Argonne National Laboratory,\\
  Lemont, IL}

% \affiliation[b]{Department, University,\\
% Street number, City, Country}

\emailAdd{foremans@anl.gov}
\emailAdd{xjin@anl.gov}
\emailAdd{osborn@alcf.anl.gov}

\abstract{
    We introduce LeapfrogLayers, an invertible neural network architecture that
    can be trained to efficiently sample the topology of a 2D $U(1)$ lattice
    gauge theory. 
    %
    We show an improvement in the integrated autocorrelation time of the
    topological charge when compared with traditional HMC, and propose methods
    for scaling our model to larger lattice volumes.
}

\FullConference{
  The 38th International Symposium on Lattice Field Theory\\
  26-30 July 2021\\
  Zoom / Gather @ MIT, Cambridge MA, USA\\
}

%% \tableofcontents
\begin{document}
\maketitle


\section{\label{sec:intro}Introduction}
%
One of the major goals of lattice field theory calculations is to evaluate
integrals of the form
%
\begin{equation}
    \langle \mathcal{O} \rangle \propto \int \left[ \mathcal{D} x \right] \mathcal{O}(x) p(x),
    \label{eq:density_integral}
\end{equation}
%
for some target distribution \(p(x)\propto e^{-S(x)}\).
%
We can approximate the integral using Markov Chain Monte Carlo (MCMC) sampling
techniques.
%
This is done by sequentially generating a chain of configurations \(\{x_{1},
x_{2}, \ldots x_{N}\}\), with \(x_{i} \sim p(x)\) and averaging the value of
the function \(\mathcal{O}(x)\) over the chain.
%
Accounting for correlations between states in our chain, the sampling variance
of this esimator is given by
%
\begin{equation}
    \sigmaup^{2} = \frac{\tau_{\mathcal{O}}^{\mathrm{int}}}{N}\sum_{n=1}^{N} \mathrm{Var}\left[\mathcal{O}(x)\right]
\end{equation}
%
% TODO: Elaborate on this
where \(\tau_{\mathrm{int}}^{\mathcal{O}}\) is the integrated autocorrelation
time.
%
This quantity can be interpreted as the additional time required for
these induced correlations to become negligible.
%
\subsection{\label{subsec:qfreezing}Charge Freezing}
%
\begin{wrapfigure}[19]{r}[36pt]{0.53\textwidth}
    \vspace{-\baselineskip}
    \centering
    \includegraphics[width=0.53\textwidth]{assets/qfreezing.pdf}
    \caption{\label{fig:qfreezing}Illustration of the topological charge \(Q\)
    freezing as \(\beta : 2 \rightarrow 7\) for traditional HMC.}
\end{wrapfigure}
%
The ability to efficiently generate independent configurations is currently
a major bottleneck for lattice simulations.
%
In this work we consider a \(U(1)\) gauge model on a 2D lattice with periodic
boundary conditions.
%
The theory is defined in terms of the link variables \(U_{\mu}(x) = e^{i
x_{\mu}(n)} \in U(1)\) with \(x_{\mu}(n) \in [-\pi, \pi]\).
%
Our target distribution is given by \(p(x)\propto e^{-S_{\beta}(x)}\), where
\(S_{\beta}(x)\) is the Wilson action
%
\begin{align}
    S_{\beta}(x) &= \beta \sum_{P} 1 - \cos{x_{P}}, \text{ and}\\
    x_{P} &\equiv \big[x_{\mu}(n) + x_{\nu}(n + \hat{\mu})\nonumber \\
          &\quad\quad\,\, - x_{\mu}(n+\hat{\nu}) - x_{\nu}(n)\big],\nonumber
\end{align}
%
is the sum of the gauge variables around the elementary plaquette.
%
Each lattice configuration has a distinct topological charge \(Q \in
\mathbb{Z}\) given by
%
\begin{align*}
    Q_{\mathbb{Z}} &= \frac{1}{2\pi}\sum_{P}\left\lfloor x_{P} \right\rfloor,\text{ where } \\
    \left\lfloor x_{P} \right\rfloor &\equiv x_{P} - 2\pi
    \left\lfloor\frac{x_{P}+\pi}{2\pi}\right\rfloor.
\end{align*}
%
As \(\beta \rightarrow \infty\), the \(Q_{\mathbb{Z}} = 0\) mode becomes
dominant and we see that the value of \(Q_\mathbb{Z}\) remains fixed for large
durations of the simulation.
%

This can be seen by introducing the \emph{tunneling rate}\footnote{As measured
between subsequent states in our chain \(i\), \(i+1\).}
%
\begin{equation}
    \delta Q \equiv \left|Q_{i+1} - Q_{i}\right| \in \mathbb{Z}.
\end{equation}
%
This quantity serves as a measure for how efficiently our chain is able to jump
(tunnel) between sectors of distinct topological charge.
%
From Fig~\ref{fig:qfreezing}, we can see that \(\delta Q\rightarrow 0\) as
\(\beta\rightarrow \infty\).
%
\section{\label{sec:hmc}Hamiltonian Monte Carlo (HMC)}
%
We first briefly review the ideas of the Hamiltonian Monte Carlo (HMC)
algorithm and highlight some of the issues faced with this approach.
%
We begin by introducing a fictitious momentum \(v \sim \mathcal{N}(0,
\mathbbm{1})\) distributed independently of \(x\).
%
This allows us to write the joint target density of the \(\xi \equiv (x, v)\)
system as
%
\begin{equation}
    p(x, v) = p(x) \cdot p(v) = e^{-S_{\beta}(x)}\cdot e^{-v^{T} v / 2} = e^{-H(x, v)}
\end{equation}
%
where \(H(\xi) = H(x, v) = S_{\beta}(x) + \frac{1}{2} v^{T} v\) is the
Hamiltonian of the system.
%

\subsection{\label{subsec:lfint}Leapfrog Integrator}
%
For a given initial state \(\xi_{0}\), we can use Hamilton's equations to
evolve the systems dynamics 
%
\begin{equation}
    \dot{x} = \frac{\partial H}{\partial v}\quad \text{,} \quad \dot{v} = -\frac{\partial H}{\partial x}
\end{equation}
%
along isoprobability contours of \(H =\text{const.}\) from \(\xi_{0} = (x_{0},
v_{0})\rightarrow (x^{\ast}, v^{\ast})\).
%
We can numerically integrate this system of equations using the \emph{leapfrog
integrator}, which is composed of the following three steps:
%
\begin{enumerate}
    \item Starting from \(x_{0}\), resample the momentum \(v_{0}\sim
        \mathcal{N}(0, \mathbbm{1})\) and construct the state \(\xi_{0} =
        (x_{0}, v_{0})\).
    \item Generate a \emph{proposal configuration} \(\xi^{\ast}\) by
        integrating \(\dot\xi\) along \(H = \mathrm{const.}\)
        for \(N\) leapfrog steps.
        i.e.
        \begin{equation}
            \xi_{0}\rightarrow \xi_{1}\rightarrow\ldots\rightarrow%
            % \xi_{N_{\mathrm{LF} - 1}}\rightarrow%
            \xi_{N} \equiv \xi^{\ast},
        \end{equation}
        where a single leapfrog step \(\xi_{i} \rightarrow \xi_{i+1}\) above
        consists of: 
        %
        \begin{equation}
            \textbf{ (a.) }%
              \tilde{v}\leftarrow v - \frac{\varepsilon}{2}\partial_{x} S(x),
            \quad\quad
            \textbf{ (b.) }%
              x' \leftarrow x + \varepsilon \tilde{v},
            \quad\quad
            \textbf{ (c.) }%
              v' \leftarrow \tilde{v} - \frac{\varepsilon}{2}\partial_{x} S(x).
        \end{equation}
        %
    \item At the end of the trajectory, accept or reject the proposal
        configuration \(\xi^{\ast}\) using the Metropolis-Hastings (MH) test.
        \begin{equation}
            x_{i+1} \leftarrow
            \begin{cases}
                x^{\ast}\text{ with probability } A(\xi^{\ast}|\xi)\\
                x_{i}\text{ with probability } 1 - A(\xi^{\ast}|\xi),
            \end{cases}
        \end{equation}
        %
        where
        %
        \begin{equation}
            A(\xi^{\ast}|\xi) \equiv \min\left\{1, \frac{p(\xi^{\ast})}{p(\xi)}\left|\frac{\partial \xi^{\ast}}{\partial \xi^{T}}\right|\right\}.
        \end{equation}
\end{enumerate}
%
An illustration of this procedure can be seen in Fig~\ref{fig:hmc}.
%
\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{assets/hmc.pdf}
    \caption{\label{fig:hmc}High-level overview of the HMC algorithm.}
\end{figure}
%
\subsection{\label{subsec:hmc_issues}Issues with HMC}
% Re-sampling the momentum at the start of each trajectory randomly selects the
% equipotential we will integrate along. energy level which is known to lead to
% slow mixing (long autocorrelations).
%
Re-sampling the momentum at the start of each trajectory causes the energy
levels we explore to be randomly selected.
%
This is reminiscient of the ``random-walk'' behavior of traditional MCMC and
leads to a slow exploration of our target distribution (i.e. long
autocorrelations).
%
% Randomly sampling the momentum at the start of each trajectory the energy level
% which is known to lead to slow mixing (long autocorrelations).
%
Additionally, the HMC sampler is known to have difficulty traversing
low-density zones, resulting in poor performance for distributions which have
multiple isolated modes.
%
This is particularly relevant in the case of sampling topological quantities in lattice gauge models.
%
% These criteria highlight the properties we would like to achieve in an ideal sampler.
% %
% Namely, we would like our sampler to: 
%
This list of criteria helps to illuminate the qualities we would like our
(ideal) sampler to exhibit; namely, we would like for our sampler to: 
%
\begin{itemize}
    \item quickly converge to the true target distribution
    \item mix quickly once converged (short autocorrelations)
    \item be able to mix across energy levels and isolated modes
\end{itemize}
%
\section{\label{sec:l2hmc}Generalizing HMC: LeapfrogLayers}
%
In order to preserve the asymptotic behavior of HMC, our update must
%
\begin{enumerate}
    \item Be invertible / preserve reversibility, i.e. \(p(a\rightarrow b) =
        p(b\rightarrow a)\)
    \item Have a tractable Jacobian determinant.
\end{enumerate}
%
To simplify notation, we introduce two functions, \(\Gamma\) \((\Lambda)\) to
denote the \(v\) \((x)\) updates.
%
As in HMC, we follow the general pattern of performing alternating updates of
\(v\) and \(x\).
%
In order to satisfy these requirements, we split the \(x\)-update
into two parts, sequentially updating complementary subsets using a binary mask
\(\mask\) and its complement \(\maskbar\).
%
Additionally, we introduce \(d \sim \mathcal{U}(+, -)\), distributed
independently of both \(x\) and \(v\), to determine the ``direction'' of our
update.
%
Here, we associate \(+\) \((-)\) with the forward (backward) direction and note
that running sequential updates in opposite directions has the effect of
inverting the update.
%
We denote the complete state by \(\xi = (x, v, \pm)\), with target density
given by \(p(\xi) = p(x)\cdot p(v)\cdot p(\pm)\).
%

Explicitly, we can write this series of updates as 
%
\begin{align}
    \mathbf{(1.)}\,\, \Gamma^{\pm} &: (v; \zeta_{v})  \rightarrow v' \\
    \mathbf{(2.)}\,\, \Lambda^{\pm} &: (\mask \odot x; \zeta_{x}) \rightarrow x' \\
    \mathbf{(3.)}\,\, \Lambda^{\pm} &: (\maskbar \odot x'; \zeta_{\bar{x'}}) \rightarrow x'' \\
    \mathbf{(4.)}\,\, \Gamma^{\pm} &: (v', \zeta_{v'})\rightarrow v''
\end{align}
%
where \(\zeta_{k}\) is independent of \(k\), and the subscript is used to
denote the variable being updated \((k = x, v)\).
%
Each of these functions are parameterized by a set of weights \(\theta\) in a
neural network.
%
Again, we proceed similar to the generic HMC leapfrog update from
Sec~\ref{subsec:lfint}, and include the high-level steps below:
%
\begin{enumerate}
    \item Resample \(v \sim \mathcal{N} (0, \mathbbm{1})\), \(d \sim
        \mathcal{U} (+, -)\), and construct initial state \(\xi_{0} = (x_{0},
        v_{0}, \pm)\)
    \item Generate the proposal configuration \(\xi^{\ast}\) by passing the
        initial state sequentially through \(N\) \emph{leapfrog
        layers}\footnote{
            We defer the details to Sec~\ref{sec:l2hmc}.
        } to construct a trajectory.
	    \begin{equation}
            \xi_{0} \rightarrow \xi_{1} \rightarrow \ldots \rightarrow%
            \xi_{N} = \xi^{\ast}
        \end{equation}
    \item Metropolis-Hastings accept/reject to determine next state in chain.
\end{enumerate}
%

%

%
% Explicitly, if we denote by \(M^{\pm} = \left(\begin{smallmatrix}\Gamma^{\pm} &
% \cdot \\ \cdot & \Lambda^{\pm}\end{smallmatrix}\right)\), we have that 
% %
% \begin{equation}
%     \xi^{\pm} = M^{\pm}\xi \Longrightarrow \xi = M^{\mp}\xi^{\pm}.
% \end{equation}
% %

\begin{thebibliography}{99}
\bibitem{...}
....

\end{thebibliography}

\end{document}
